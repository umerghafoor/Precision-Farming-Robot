{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scaled-Down Precision Farming Robot","text":"<p>Welcome to the official documentation for the Scaled-Down Precision Farming Robot. This project aims to develop an automated solution for efficient weed detection and targeting using basic image processing techniques. By leveraging the power of robotics and IoT, this robot minimizes the need for manual labor and reduces herbicide use, contributing to more sustainable farming practices.</p>"},{"location":"#overview","title":"Overview","text":"<p>The robot is designed to operate in controlled environments, detecting and targeting simulated weeds represented by simple patterns. It consists of two main modules:</p> <ul> <li>Rover Module: Responsible for the robot\u2019s movement and navigation.</li> <li>Header Module: Houses the camera, servo motor, and laser pointer for accurate weed detection and targeting.</li> </ul> <p>The system integrates various technologies:</p> <ul> <li>MQTT: For communication between the robot and applications.</li> <li>OpenCV: For image processing and weed detection.</li> <li>Mobile/Desktop Applications: For user control and monitoring.</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Weed Detection and Targeting: The robot automatically detects weeds using image processing techniques and accurately targets them with a laser pointer.</li> <li>Modular System Design: The robot\u2019s components are divided into two modules\u2014the Rover (for movement) and the Header (for weed detection and targeting)\u2014making it easy to upgrade and modify each module independently.</li> <li>Real-time Communication: The system uses the MQTT protocol for seamless data transfer between the robot and external applications, ensuring real-time control and monitoring.</li> <li>Custom Applications: Users can control and monitor the robot using custom-built desktop applications (PyQt6) and mobile applications (Flutter).</li> <li>Scalable Prototype: The robot is built with lightweight materials, allowing for easy scaling and future upgrades.</li> <li>Sustainability: By automating the detection of weeds and reducing herbicide use, this project promotes eco-friendly farming practices.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started with the project, follow the Setup Instructions for hardware assembly, software installation, and initial testing.</p>"},{"location":"#applications","title":"Applications","text":"<p>This project serves as a foundation for future advancements in precision farming and eco-friendly agricultural technologies. The robot can be enhanced and scaled to include:</p> <ul> <li>Advanced Machine Learning Algorithms: For improved weed detection accuracy and adaptability to various environments.</li> <li>Autonomous Navigation: For full automation of the farming process.</li> <li>Real-World Agricultural Applications: Extending the technology for use in real agricultural fields to reduce herbicide use and improve efficiency.</li> </ul>"},{"location":"#contributions","title":"Contributions","text":"<p>This project is open-source, and we welcome contributions from developers, engineers, and researchers. Whether you\u2019re interested in improving the existing design, adding new features, or exploring novel applications, we\u2019d love to collaborate with you.</p> <p>Check out our GitHub Repository for the full source code and further documentation. Feel free to reach out for collaboration or improvement suggestions.</p>"},{"location":"#paper","title":"Paper","text":"<p>For further academic insights into the development of this precision farming robot, refer to the paper on ResearchGate, which details the methodology, design, and testing process of the robot.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for more information.</p>"},{"location":"Features/","title":"Features","text":""},{"location":"Features/#1-weed-detection-and-targeting","title":"1. Weed Detection and Targeting","text":"<p>The system employs basic image processing techniques, such as thresholding, to detect weeds represented by simple patterns like dots or marks in the camera feed. Once a target is identified, the robot's laser pointer is used for precise alignment, ensuring accurate targeting. This ability to pinpoint weeds allows for efficient and automated intervention, reducing the need for manual labor or herbicide application in the future phases of development.</p>"},{"location":"Features/#2-modular-system-design","title":"2. Modular System Design","text":"<p>The project is built around a modular architecture consisting of two primary components: the Rover and the Header. The Rover, equipped with DC or stepper motors, provides mobility, while the Header carries the camera, servo motor, and laser pointer. This separation of concerns allows each module to function independently, enabling parallel development and easy upgrades. This design enhances system flexibility, making it easier to integrate advanced sensors, machine learning algorithms, or GPS technology as the project progresses.</p>"},{"location":"Features/#3-communication-and-control","title":"3. Communication and Control","text":"<p>To ensure seamless communication between the robot and the control systems, the project uses the MQTT protocol, a lightweight messaging protocol suited for IoT applications. Data transmission occurs through two main topics: the Video Topic, which streams the camera feed in UTF-8 format, and the Control Topic, which transmits control commands in JSON format. This enables efficient data exchange with minimal latency, supporting real-time operation and control of the robot from both desktop and mobile applications.</p>"},{"location":"Features/#4-custom-applications","title":"4. Custom Applications","text":"<p>The project is supported by two custom applications that offer easy control and monitoring. The Desktop Application is developed using PyQt6 and provides a simple interface for manual control of the robot\u2019s movement, as well as task scheduling functionalities. The Mobile Application, built using Flutter, allows users to stream live video from the robot\u2019s camera, enabling on-the-go monitoring. Both applications connect to the robot via MQTT, offering real-time feedback and control from anywhere.</p>"},{"location":"Features/#5-compact-and-scalable-prototype","title":"5. Compact and Scalable Prototype","text":"<p>The robot is designed to be both compact and scalable. The chassis is constructed using lightweight aluminum and wood, providing a stable yet portable base for the components. While the prototype currently focuses on basic navigation and weed detection, the modular approach ensures scalability, enabling easy upgrades for advanced navigation, machine learning-based weed identification, and autonomous operation. The scalable nature of the design allows it to evolve into a fully functional precision farming robot for real-world applications.</p>"},{"location":"Features/#6-hardware-integration","title":"6. Hardware Integration","text":"<p>The robot is powered by an ESP32 microcontroller, which manages motor and servo control, as well as communication via MQTT. The L298N motor driver ensures smooth movement for the Rover by controlling the motors with precise speed and direction. Additionally, the laser pointer\u2019s alignment is controlled through a servo motor, making the system capable of fine-tuning its targeting precision. This hardware integration enables effective navigation and targeting, forming the backbone of the robot\u2019s functionality.</p>"},{"location":"Features/#7-sustainability-and-innovation","title":"7. Sustainability and Innovation","text":"<p>One of the key goals of this project is to support sustainable farming practices by reducing the reliance on herbicides. By automating weed detection and targeting, the system minimizes the environmental impact associated with traditional weed management methods. This approach not only improves resource efficiency but also serves as a foundation for future innovations in precision farming. The system is designed to be adaptable, allowing for the incorporation of more advanced technologies as they become available.</p>"},{"location":"Features/#8-results-oriented-functionality","title":"8. Results-Oriented Functionality","text":"<p>The system has been successfully tested in controlled environments, demonstrating accurate movement and targeting capabilities. The robot\u2019s ability to follow control commands and adjust the laser pointer to target weeds has been validated. Furthermore, real-time video streaming and control commands were transmitted seamlessly through MQTT, ensuring a smooth user experience with minimal latency. These results indicate that the prototype meets its objectives, establishing a solid foundation for further development and refinement.</p>"},{"location":"Setup-Instructions/","title":"Setup Instructions","text":""},{"location":"Setup-Instructions/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Directory structure</li> <li>Hardware Setup</li> <li>Motor Connection</li> <li>Servo Motor for Laser Alignment</li> <li>Camera and Laser Pointer Installation</li> <li>Secure Mounting of Components</li> <li>Final Checks</li> <li>Software Setup</li> <li>Install Required Software</li> <li>Upload Code to ESP32</li> <li>Desktop Application Setup</li> <li>Testing and Calibration</li> <li>Initial Testing</li> <li>Calibrate Components</li> <li>Troubleshooting</li> </ol>"},{"location":"Setup-Instructions/#1-directory-structure","title":"1. Directory structure","text":"<pre><code>\u2514\u2500\u2500 Precision-Farming-Robot/\n    \u251c\u2500\u2500 README.md\n    \u251c\u2500\u2500 LICENSE\n    \u251c\u2500\u2500 Application/\n    \u2502   \u251c\u2500\u2500 readme.md\n    \u2502   \u251c\u2500\u2500 ControllerWidget.py\n    \u2502   \u251c\u2500\u2500 main.py\n    \u2502   \u2514\u2500\u2500 style.qss\n    \u251c\u2500\u2500 Firmware/\n    \u2502   \u251c\u2500\u2500 readme.md\n    \u2502   \u251c\u2500\u2500 platformio.ini\n    \u2502   \u251c\u2500\u2500 .gitignore\n    \u2502   \u251c\u2500\u2500 include/\n    \u2502   \u2502   \u251c\u2500\u2500 README\n    \u2502   \u2502   \u251c\u2500\u2500 constants.h\n    \u2502   \u2502   \u251c\u2500\u2500 controller.h\n    \u2502   \u2502   \u2514\u2500\u2500 mqtt_broker.h\n    \u2502   \u251c\u2500\u2500 lib/\n    \u2502   \u2502   \u2514\u2500\u2500 README\n    \u2502   \u251c\u2500\u2500 src/\n    \u2502   \u2502   \u251c\u2500\u2500 controller.cpp\n    \u2502   \u2502   \u251c\u2500\u2500 main.cpp\n    \u2502   \u2502   \u2514\u2500\u2500 mqtt_broker.cpp\n    \u2502   \u251c\u2500\u2500 test/\n    \u2502   \u2502   \u2514\u2500\u2500 README\n    \u2502   \u2514\u2500\u2500 .vscode/\n    \u2502       \u251c\u2500\u2500 extensions.json\n    \u2502       \u2514\u2500\u2500 settings.json\n    \u251c\u2500\u2500 Hardware/\n    \u2502   \u2514\u2500\u2500 readme.md\n    \u2514\u2500\u2500 Software/\n        \u251c\u2500\u2500 readme.md\n        \u251c\u2500\u2500 aruco_mark_detection_module.py\n        \u251c\u2500\u2500 dot_detection_module.py\n        \u251c\u2500\u2500 dot_detection_module_mqtt._sub.py\n        \u251c\u2500\u2500 main.py\n        \u251c\u2500\u2500 pub.py\n        \u251c\u2500\u2500 sub.py\n        \u2514\u2500\u2500 utility.py\n</code></pre>"},{"location":"Setup-Instructions/#2-hardware-setup","title":"2. Hardware Setup","text":""},{"location":"Setup-Instructions/#1-motor-connection","title":"1. Motor Connection","text":"<ul> <li>Attaching Motors to the Chassis:     Secure the motors to the chassis using appropriate motor brackets and screws. Ensure the motors are positioned properly to allow smooth movement. Verify that the motor shafts align with the wheels or any other mechanical components you intend to use for movement.</li> <li> <p>Wiring the Motors to the Motor Driver:     Connect the motors to the L298N motor driver as follows:</p> <p>Motor 1:   - <code>ENA_M1 (21)</code> for speed control (PWM).   - <code>IN1_M1 (22)</code> and <code>IN2_M1 (23)</code> to control direction (high/low).</p> <p>Motor 2:   - <code>ENB_M2 (5)</code> for speed control (PWM).   - <code>IN3_M2 (18)</code> and <code>IN4_M2 (19)</code> to control direction.</p> <p>Motor 3:   - <code>ENA_M3 (15)</code> for speed control.   - <code>IN1_M3 (0)</code> and <code>IN2_M3 (2)</code> for direction control.</p> <p>Motor 4:   - <code>ENB_M4 (17)</code> for speed control.   - <code>IN3_M4 (16)</code> and <code>IN4_M4 (4)</code> for direction control.</p> </li> </ul> <p>These connections ensure that each motor can be individually controlled for precise movement and steering.</p>"},{"location":"Setup-Instructions/#2-servo-motor-for-laser-alignment","title":"2. Servo Motor for Laser Alignment","text":"<ul> <li>Mounting the Servo Motor:     Attach the servo motor securely to the header module. Use screws or mounting brackets to ensure it stays in place during movement. The servo will be responsible for adjusting the angle of the laser, so make sure it's positioned to allow full rotation or adjustment.</li> <li>Connecting the Servo:     Wire the servo to the <code>SERVO_PIN (32)</code> on the ESP32. This pin will control the servo's rotation, allowing you to adjust the laser's alignment based on feedback or manual control.</li> </ul>"},{"location":"Setup-Instructions/#3-camera-and-laser-pointer-installation","title":"3. Camera and Laser Pointer Installation","text":"<ul> <li>Mounting the Camera:     Fix the camera onto the header module, ensuring it's aligned with the servo's range of motion. The camera should be positioned to capture the area in front of the robot, offering a clear view for any computer vision tasks or object detection applications.</li> <li>Aligning the Laser Pointer:     Attach the laser pointer to the same header module, making sure it's aligned with the camera. This alignment ensures that the laser can be used for accurate targeting or positioning. Double-check the alignment by testing the laser's focus and direction after the components are mounted.</li> <li>Securing Components:     All components, including the camera and laser pointer, should be securely fastened to avoid any shifting during movement. You can use mounting brackets, Velcro strips, or additional screws to ensure they remain in place.</li> </ul>"},{"location":"Setup-Instructions/#4-secure-mounting-of-components","title":"4. Secure Mounting of Components","text":"<ul> <li> <p>General Tips for Mounting:      Ensure all components are securely mounted on the chassis. Loose components can cause instability during movement or affect performance. Use high-strength adhesives or screws for mounting, particularly for heavier components like the camera or motor driver.</p> </li> <li> <p>Balance and Weight Distribution:      Pay attention to the distribution of weight across the robot. Ideally, the center of gravity should be low and centered to improve stability. Distribute the components evenly to avoid tipping or erratic movements.</p> </li> <li> <p>Cable Management:      Carefully manage the wires between the components to avoid tangling or interference with moving parts. Use zip ties, cable clips, or channels to keep wires organized and out of the way. Ensure that the wires are long enough to reach each component without strain but short enough to avoid slack.</p> </li> </ul>"},{"location":"Setup-Instructions/#5-final-checks","title":"5. Final Checks","text":"<ul> <li> <p>Power Supply:      Ensure that the power supply can provide enough current to operate all the components (motors, ESP32, servo, camera, laser). If necessary, use a separate power source for motors and logic circuits to prevent power drops that could affect performance.</p> </li> <li> <p>Testing the System:      After mounting everything, run a series of tests. First, check the movement of the motors and servo to ensure correct wiring. Then, test the camera and laser alignment by activating the servo to move the laser and confirm it remains properly aligned with the camera.</p> </li> </ul> <p>Once all components are mounted, securely connected, and tested, your robot should be ready for further programming and functionality enhancements!</p>"},{"location":"Setup-Instructions/#3-software-setup","title":"3. Software Setup","text":""},{"location":"Setup-Instructions/#1-install-required-software","title":"1. Install Required Software","text":"<ul> <li> <p>Mosquitto MQTT Broker:      Download and install Mosquitto to set up a local MQTT server. Follow these steps:</p> <ol> <li>Visit the Mosquitto website to download the latest version for your operating system.</li> <li>Follow the installation instructions provided on the website.</li> <li> <p>Once installed, start the Mosquitto service by running the command:</p> <p><code>bash mosquitto</code></p> </li> <li> <p>This will start the broker on the default port (1883), allowing devices to communicate over MQTT.</p> </li> </ol> </li> <li> <p>Python and Libraries</p> </li> <li> <p>Install Python (version 3.x or above) on your machine from the official Python website.</p> </li> <li> <p>Install the required libraries for your desktop application by running:</p> <p><code>bash pip install -r requirements.txt</code></p> <p>Ensure that your <code>requirements.txt</code> file includes the necessary dependencies like:</p> </li> <li> <p><code>OpenCV</code>: For camera processing.</p> </li> <li><code>Paho MQTT</code>: For MQTT communication.</li> <li><code>PyQt6</code>: For building the desktop GUI.</li> </ul>"},{"location":"Setup-Instructions/#2-upload-code-to-esp32","title":"2. Upload Code to ESP32","text":"<ul> <li> <p>Using Arduino IDE      If you're using Arduino IDE:</p> <ol> <li>Open Arduino IDE.</li> <li>Select the ESP32 board:<ul> <li>Go to Tools &gt; Board &gt; ESP32 Dev Module (or the specific ESP32 board you are using).</li> </ul> </li> <li>Select the correct COM port:<ul> <li>Go to Tools &gt; Port and select the appropriate port for your ESP32.</li> </ul> </li> <li>Upload the code:<ul> <li>Make sure the code is properly configured to control motors and communicate via MQTT. Ensure that the Wi-Fi credentials and MQTT broker settings are correctly defined in the code.</li> <li>Click Upload to flash the code to the ESP32.</li> </ul> </li> </ol> </li> <li> <p>Using PlatformIO      If you're using PlatformIO (within VSCode):</p> <ol> <li>Open VSCode and the PlatformIO extension.</li> <li>In platformio.ini, ensure that the ESP32 board is selected.</li> <li>Connect the ESP32 and select the correct COM port:<ul> <li>PlatformIO will detect the connected device, but if necessary, you can specify the port manually in the <code>platformio.ini</code> file.</li> </ul> </li> <li> <p>Upload the code:</p> <ul> <li>Make sure the code is set up for controlling the motors and communication over MQTT, including the correct Wi-Fi and MQTT broker settings.</li> <li>Click the Upload button in PlatformIO or run the following command in the terminal:</li> </ul> <p><code>bash   pio run --target upload</code></p> </li> </ol> </li> <li> <p>Verify Settings in Code</p> </li> </ul> <p>Ensure that the following settings are correctly configured:</p> <ul> <li> <p>Wi-Fi credentials: Make sure the SSID and password are specified correctly for the network.</p> </li> <li> <p>MQTT settings: Verify that the IP address of the Mosquitto broker is set correctly, along with the MQTT topics for communication. For example:</p> </li> </ul> <pre><code>#define MQTT_SERVER \"192.168.18.29\" // Replace with your broker's IP\n#define MQTT_PORT 1883\n#define SUBSCRIBE_TOPIC \"robot/control\"\n#define SUBSCRIBE_TOPIC_Pointer \"robot/pointer\"\n#define PUBLISH_TOPIC \"robot/status\"\n</code></pre> <p>Once everything is uploaded and set up, the ESP32 will be ready to communicate with the MQTT broker, control the motors, and interact with the desktop application.</p>"},{"location":"Setup-Instructions/#3-desktop-application-setup","title":"3. Desktop Application Setup","text":"<ul> <li>PyQt6 Application:</li> <li>Open Application folder inside the main repo.</li> <li>Run the application by executing <code>python main.py</code> or the appropriate start command.  </li> <li>Ensure the application is connected to the MQTT broker for sending control commands.  </li> <li>Check the user interface for manual control options and task scheduling.  </li> </ul>"},{"location":"Setup-Instructions/#4-testing-and-calibration","title":"4. Testing and Calibration","text":""},{"location":"Setup-Instructions/#1-initial-testing","title":"1. Initial Testing","text":"<ul> <li>After completing the hardware and software setup, power on the robot.  </li> <li>Open the desktop application to begin controlling the robot.  </li> <li>Verify that the robot can move, and the camera feed is visible on the control interface.  </li> <li>Check if the laser pointer aligns with detected targets accurately.</li> </ul>"},{"location":"Setup-Instructions/#2-calibrate-components","title":"2. Calibrate Components","text":"<p>Here\u2019s how you can approach the calibration and configuration of the components in your project:</p> <ul> <li>Adjust the Servo Motor for Proper Targeting:     The servo motor needs to be calibrated for precise targeting. This involves adjusting the servo to point the laser or camera at the target area.</li> <li>If you have a known target, use the servo to move the laser or camera to the optimal position and verify if the target is correctly aligned with the camera's field of view.</li> <li> <p>Use the DISTANCE value to adjust the height of the servo and camera. Fine-tuning the servo will ensure the laser is pointing in the correct direction for effective weed detection.</p> </li> <li> <p>Fine-Tune Image Processing Settings:     To achieve optimal weed detection, you may need to adjust the image processing settings, particularly the thresholding parameters.</p> </li> <li>Thresholding is essential for distinguishing between weeds and the background in the image. Fine-tune the threshold values based on the lighting conditions and the appearance of weeds.</li> <li> <p>In your Python script (<code>main.py</code>), adjust the DETECTION_MODE parameter to select the appropriate mode for weed detection:</p> <ul> <li>\"CIRCLE\": Detect circular objects (useful for round weeds).</li> <li>\"SHAPE\": Detect specific shapes (useful for recognizing weeds with particular geometric characteristics).</li> <li>\"ARUCO\": Use ARUCO markers for detection (if you're using specific markers).</li> <li>You can also adjust the thresholding and other processing parameters to improve the accuracy of weed detection.</li> </ul> </li> <li> <p>Configuring the Python Script (<code>main.py</code>): In your <code>main.py</code> file, set the values for the components and detection:</p> </li> </ul> <pre><code># Configuration variables\nDISTANCE = 27  # Height of the servo and camera in cm (adjust as needed)\nSCALAR = 30  # Scale factor for processing or distance adjustment\nBROKER = \"192.168.18.29\"  # Replace with your MQTT broker address\nPORT = 1883  # MQTT broker port\nTOPIC = \"robot/pointer\"  # MQTT topic for controlling the pointer/laser\nTOPIC_VIDEO = \"video/stream\"  # MQTT topic for video stream\nDETECTION_MODE = \"CIRCLE\"  # \"CIRCLE\", \"SHAPE\", \"ARUCO\" - Choose detection method\nVIDEO_WIDTH = 640  # Width of video feed\nVIDEO_HEIGHT = 360  # Height of video feed\n</code></pre>"},{"location":"Setup-Instructions/#steps-to-calibrate","title":"Steps to Calibrate","text":"<ol> <li>Set the correct <code>DISTANCE</code> value:</li> <li>This defines the height of the servo and camera. Adjust the value based on the actual physical height from the ground or platform to where the camera and servo are mounted.</li> <li> <p>Example: If the camera and servo are mounted 27 cm above the surface, set <code>DISTANCE = 27</code>.</p> </li> <li> <p>Fine-tune the <code>SCALAR</code> value:  </p> </li> <li> <p>The SCALAR variable could be used for adjusting the scale in calculations or for determining the relative distance to the target. This might be useful for distance-based adjustments or zoom levels in image processing.</p> </li> <li> <p>Set up the MQTT broker configuration:  </p> </li> <li>Replace the <code>BROKER</code> value with the IP address of your Mosquitto MQTT broker (e.g., <code>BROKER = \"192.168.18.29\"</code>).</li> <li> <p>Make sure the MQTT server is running and accessible from the robot.</p> </li> <li> <p>Configure Detection Mode:  </p> </li> <li> <p>The <code>DETECTION_MODE</code> can be set to different modes depending on what kind of detection you want:</p> <ul> <li><code>\"CIRCLE\"</code>: Detects circular objects (ideal for circular weeds).</li> <li><code>\"SHAPE\"</code>: Detects shapes based on custom algorithms (ideal for irregularly shaped weeds).</li> <li><code>\"ARUCO\"</code>: Detects ARUCO markers (if you're using markers for precise positioning).</li> </ul> </li> <li> <p>Adjust Video Feed Resolution:  </p> </li> <li>The <code>VIDEO_WIDTH</code> and <code>VIDEO_HEIGHT</code> values determine the resolution of the video stream. For example, setting <code>VIDEO_WIDTH = 640</code> and <code>VIDEO_HEIGHT = 360</code> gives a moderate resolution that balances performance and quality.</li> </ol>"},{"location":"Setup-Instructions/#final-steps","title":"Final Steps","text":"<ul> <li>Test the calibration by running the system and checking if the servo motor and camera are correctly aligned and focused on the target area.</li> <li>Fine-tune the image processing thresholds to make sure the weeds are detected accurately under different lighting conditions.</li> </ul>"},{"location":"Setup-Instructions/#5-troubleshooting","title":"5. Troubleshooting","text":"<ul> <li>Connection Issues:  </li> <li>Ensure the MQTT broker is running and accessible.  </li> <li>Verify the Wi-Fi connection for the ESP32.  </li> <li> <p>Check the MQTT topic configurations in the code.  </p> </li> <li> <p>Hardware Issues:  </p> </li> <li>Ensure that the motors are wired correctly to the driver and that the power supply is sufficient.  </li> <li>Test the servo motor and camera separately to ensure they are functioning properly.</li> </ul>"}]}